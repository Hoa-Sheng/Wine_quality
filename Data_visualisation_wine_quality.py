"""Projet de CONG Hoa-sheng

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lwyVlIr_OluKHuVqHoXUu3VyJ3zw6ftQ
"""

# **B. Projections de données et Visualisation**

# **1)**

from sklearn.decomposition import PCA
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df = pd.read_csv('winequality-red.csv', delimiter= ',')


print(df)




pca = PCA(n_components=2)
proj2d = pca.fit_transform(df)


plt.scatter(proj2d[:,0], proj2d[:,1])
plt.title("Projections en 2D")
plt.show()

pca = PCA(n_components=3)
proj3d = pca.fit_transform(df)

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(proj3d[:,0], proj3d[:,1], proj3d[:,2])
ax.set_title("Projections en 3D")
plt.show()

"""Grace au PCA nous pouvons voir où est ce que nous avons une grande densité de donnée."""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error

X=pd.DataFrame(proj3d)
y = df.values



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

r2 = r2_score(y_test, y_pred)
print(f"R2 Score: {r2:.3f}")

mse = np.mean((y_test - y_pred) ** 2)
print('mse:', mse)

"""t-SNE :"""

from sklearn.manifold import TSNE

tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(df)
plt.scatter(X_tsne[:,0], X_tsne[:,1])
plt.title("Projections en 2D")
plt.show()

tsne = TSNE(n_components=3)
X_tsne = tsne.fit_transform(df)
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(X_tsne[:,0], X_tsne[:,1], X_tsne[:,2])
ax.set_title("Projections en 3D")
plt.show()

X=pd.DataFrame(X_tsne)
y = df.values


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

r2 = r2_score(y_test, y_pred)
print(f"R2 Score: {r2:.3f}")

mse = np.mean((y_test - y_pred) ** 2)
print('mse:', mse)

"""LLE :

"""

from sklearn.manifold import LocallyLinearEmbedding


lle = LocallyLinearEmbedding(n_components=2)
X_lle = lle.fit_transform(df)

plt.scatter(X_lle[:, 0], X_lle[:, 1])
plt.title('LLE 2D')
plt.show()

lle = LocallyLinearEmbedding(n_components=3)
X_lle = lle.fit_transform(df)

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(X_lle[:, 0], X_lle[:, 1], X_lle[:, 2])
plt.title('LLE 3D')
plt.show()

X=pd.DataFrame(X_lle)
y = df.values



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

r2 = r2_score(y_test, y_pred)
print(f"R2 Score: {r2:.3f}")

mse = np.mean((y_test - y_pred) ** 2)
print('mse:', mse)

"""Grace a ces 3 méthodes nous pouvons voir où est ce que nous avons une grande densité de donnée. Grace a un coefficient de determination elevé et a une erreur quadratique moyenne faible nous pourrions penser que le PCA est la methode la plus precise dans notre cas. Mais graphiquement le délimitation est meilleure avec le t-SNE.

# **5)**

Pour améliorer les résultats de la projection, il est suggéré d'utiliser des méthodes plus avancées de réduction de dimensionnalité, d'essayer différentes techniques de clustering, d'explorer différentes combinaisons de paramètres, d'utiliser des techniques de visualisation supplémentaires, de prétraiter les données, etc. Grace à ça, il est possible d'obtenir des résultats plus précis et une meilleure compréhension des structures dans les données.

# **Partie 3**

# **1)**
"""

X = df["fixed_acidity"].values
y = df["citric_acid"].values

X = X.reshape(-1, 1)
y = y.reshape(-1, 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LinearRegression()

model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("Erreur quadratique moyenne (MSE) :", mean_squared_error(y_test, y_pred))
print("Coefficient de détermination (R2) :", r2_score(y_test, y_pred))



plt.scatter(X_test, y_test, color="black")
plt.plot(X_test, y_pred, color="blue", linewidth=3)

"""La ligne de tendance suit bien la tendance des points, cela indique une bonne correspondance entre la variable "citric acid" et la variable "fixed acidity". On peut apercevoir une augmentation. A terme, on peut conclure que plus il y a d'acide citrique, plus la quantité d'acide qui se fixe est importante"""

X = df["fixed_acidity"].values
y = df["volatile_acidity"].values

X = X.reshape(-1, 1)
y = y.reshape(-1, 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LinearRegression()

model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("Erreur quadratique moyenne (MSE) :", mean_squared_error(y_test, y_pred))
print("Coefficient de détermination (R2) :", r2_score(y_test, y_pred))



plt.scatter(X_test, y_test, color="black")
plt.plot(X_test, y_pred, color="blue", linewidth=3)

"""La ligne de tendance suit bien la tendance des points, cela indique une bonne correspondance entre la variable "volatil acidity" et la variable "fixed acidity". n peut voir que plus l'acidité qui se fixe augmente, moins l'acide devient volatile.

# **2)**
"""

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = GaussianNB()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
f1=f1_score(y_test, y_pred, average='macro')
recall=recall_score(y_test, y_pred, average='macro')

print("Accuracy score :", accuracy)
print('Precision:' , precision)
print('F1 Score:',f1)
print('Recall:',recall)

"""Grâce a ces fonctions on voit que les données sont plutot exact mais pas précise."""

import seaborn as sns
sns.pairplot(df)

"""Grace a cet visualisation nous voyons que variable ont une correlation entre elle comme free_sulfur_dioxyde et total_dioxyde alors que aucune variable n'est correllé a la quality. On peut ainsi en deduire des tendances. Et des relations entre variables."""